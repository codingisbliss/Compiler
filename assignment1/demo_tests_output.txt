tests/test1_all_tokens.txt
=======================

First token peeked:
1:1 (0-75) TokenType.COMMENT "*** this test file contains all the legal tokens in the language, I believe"

All tokens: (line:col (begin-end) TYPE "value")
1:1 (0-75) TokenType.COMMENT "*** this test file contains all the legal tokens in the language, I believe"
2:1 (76-97) TokenType.COMMENT "*** this is a comment"
3:1 (98-114) TokenType.COMMENT "*** all keywords"
4:1 (115-120) TokenType.KW_ARRAY "array"
5:1 (121-126) TokenType.KW_DEFUN "defun"
6:1 (127-129) TokenType.KW_DO "do"
7:1 (130-134) TokenType.KW_ELSE "else"
8:1 (135-140) TokenType.KW_ELSIF "elsif"
9:1 (141-144) TokenType.KW_END "end"
10:1 (145-152) TokenType.KW_FOREACH "foreach"
11:1 (153-159) TokenType.KW_GLOBAL "global"
12:1 (160-162) TokenType.KW_IF "if"
13:1 (163-165) TokenType.KW_IN "in"
14:1 (166-171) TokenType.KW_LOCAL "local"
15:1 (172-177) TokenType.PRINT "print"
16:1 (178-184) TokenType.RETURN "return"
17:1 (185-189) TokenType.KW_THEN "then"
18:1 (190-195) TokenType.KW_TUPLE "tuple"
19:1 (196-201) TokenType.KW_WHILE "while"
20:1 (202-241) TokenType.COMMENT "*** all operators and other punctuation"
21:1 (242-244) TokenType.OP_NOTEQUA "!="
22:1 (245-246) TokenType.LPAR "("
23:1 (247-248) TokenType.RPAR ")"
24:1 (249-250) TokenType.OP_MULT "*"
25:1 (251-252) TokenType.OP_PLUS "+"
26:1 (253-254) TokenType.OP_COMMA ","
27:1 (255-256) TokenType.OP_MINUS "-"
28:1 (257-258) TokenType.OP_DOT "."
29:1 (259-261) TokenType.OP_DOTDOT ".."
30:1 (262-263) TokenType.OP_DIV "/"
31:1 (264-265) TokenType.SEMI ";"
32:1 (266-267) TokenType.OP_LESS "<"
33:1 (268-271) TokenType.EXCHANGE "<->"
34:1 (272-274) TokenType.OP_LESSEQUAL "<="
35:1 (275-276) TokenType.ASSIGN "="
36:1 (277-279) TokenType.OP_EQUAL "=="
37:1 (280-281) TokenType.OP_GREATER ">"
38:1 (282-284) TokenType.OP_GREATEREQUAL ">="
39:1 (285-286) TokenType.LBRAK "["
40:1 (287-288) TokenType.RBRAK "]"
41:1 (289-300) TokenType.COMMENT "*** numbers"
42:1 (301-302) TokenType.INT_LIT "0"
43:1 (303-306) TokenType.INT_LIT "000"
44:1 (307-309) TokenType.INT_LIT "30"
45:1 (310-312) TokenType.INT_LIT "47"
46:1 (313-334) TokenType.COMMENT "*** too large a value"
47:1-47:23 Warning: integer "9999999999999999999999" will be clamped to 2^31-1.
47:1 (335-357) TokenType.INT_LIT "2147483647"
48:1 (358-427) TokenType.COMMENT "*** you decide what to do with this: the number is long but not large"
49:1 (428-450) TokenType.INT_LIT "0000000000000000000000"
50:1 (451-457) TokenType.COMMENT "*** ID"
51:1 (458-459) TokenType.ID "_"
52:1 (460-461) TokenType.ID "a"
53:1 (462-464) TokenType.ID "aa"
54:1 (465-468) TokenType.ID "aba"
55:1 (469-474) TokenType.ID "abdac"
56:1 (475-483) TokenType.ID "_I_am_ID"
57:1 (484-515) TokenType.ID "_I_might_be_considered_too_long"
57:33 (516-559) TokenType.COMMENT "*** OK to produce a warning here, or not to"
58:1 (560-599) TokenType.COMMENT "*** the ID below is DEFINITELY too long"
59:1-59:94 Warning: identifier "this_is_a_definitely_way_too_long_ID_no_questions_about_it_yeah_yeah_yeah_yeah_yeah_yeah_yeah" will be truncated to 80 characters.
59:1 (600-693) TokenType.ID "this_is_a_definitely_way_too_long_ID_no_questions_about_it_yeah_yeah_yeah_yeah_y"
Done.  Scanned 60 tokens.


tests/test2_garbage.txt
=======================

First token peeked:
1:1 (0-18) TokenType.COMMENT "*** scanner test 2"

All tokens: (line:col (begin-end) TYPE "value")
1:1 (0-18) TokenType.COMMENT "*** scanner test 2"
2:1 (19-94) TokenType.COMMENT "*** this comment contains something that looks like legal code: a=1; b=a+b;"
3:1 (95-132) TokenType.COMMENT "*** but it's still just a comment!!!!"
5:1 (134-161) TokenType.COMMENT "*** this is just garbage..."
6:1-6:2 Warning: unrecognized character "#".
6:2-6:3 Warning: unrecognized character "#".
6:3-6:4 Warning: unrecognized character "#".
6:4-6:5 Warning: unrecognized character "#".
6:5-6:6 Warning: unrecognized character "!".
6:6-6:7 Warning: unrecognized character "@".
6:7-6:8 Warning: unrecognized character "#".
6:8-6:9 Warning: unrecognized character "%".
6:9-6:10 Warning: unrecognized character "^".
6:10-6:11 Warning: unrecognized character "}".
6:11-6:12 Warning: unrecognized character "}".
6:12-6:13 Warning: unrecognized character "{".
7:1-7:2 Warning: unrecognized character "\".
8:1-8:2 Warning: unrecognized character "\".
8:2-8:3 Warning: unrecognized character "\".
8:3-8:4 Warning: unrecognized character "!".
8:4-8:5 Warning: unrecognized character "#".
8:5-8:6 Warning: unrecognized character "@".
8:6-8:7 Warning: unrecognized character "!".
8:7-8:8 Warning: unrecognized character "$".
9:1 (185-203) TokenType.COMMENT "*** end of garbage"
Done.  Scanned 5 tokens.


tests/test3_outline.txt
=======================

First token peeked:
1:1 (0-25) TokenType.COMMENT "*** From outline.txt file"

All tokens: (line:col (begin-end) TYPE "value")
1:1 (0-25) TokenType.COMMENT "*** From outline.txt file"
3:1 (27-46) TokenType.COMMENT "*** Calculator only"
4:1 (47-52) TokenType.PRINT "print"
4:7 (53-56) TokenType.INT_LIT "123"
4:10 (56-57) TokenType.OP_MULT "*"
4:11 (57-60) TokenType.INT_LIT "231"
4:14 (60-61) TokenType.OP_PLUS "+"
4:15 (61-62) TokenType.LPAR "("
4:16 (62-65) TokenType.INT_LIT "123"
4:19 (65-66) TokenType.OP_DIV "/"
4:20 (66-68) TokenType.INT_LIT "45"
4:22 (68-69) TokenType.RPAR ")"
5:1 (70-103) TokenType.COMMENT "*** some variables and calculator"
6:1 (104-110) TokenType.KW_GLOBAL "global"
6:8 (111-112) TokenType.ID "a"
6:9 (112-113) TokenType.ASSIGN "="
6:10 (113-115) TokenType.INT_LIT "10"
7:1 (116-121) TokenType.PRINT "print"
7:7 (122-123) TokenType.ID "a"
7:8 (123-124) TokenType.OP_MULT "*"
7:9 (124-125) TokenType.ID "a"
8:1 (126-140) TokenType.COMMENT "*** a function"
9:1 (141-146) TokenType.KW_DEFUN "defun"
9:7 (147-153) TokenType.ID "square"
9:14 (154-155) TokenType.LPAR "("
9:15 (155-156) TokenType.ID "x"
9:16 (156-157) TokenType.RPAR ")"
9:18 (158-164) TokenType.RETURN "return"
9:25 (165-166) TokenType.ID "x"
9:26 (166-167) TokenType.OP_MULT "*"
9:27 (167-168) TokenType.ID "x"
9:28 (168-169) TokenType.SEMI ";"
9:30 (170-173) TokenType.KW_END "end"
9:34 (174-179) TokenType.KW_DEFUN "defun"
10:1 (180-186) TokenType.KW_GLOBAL "global"
10:8 (187-188) TokenType.ID "b"
10:9 (188-189) TokenType.ASSIGN "="
10:10 (189-191) TokenType.INT_LIT "12"
11:1 (192-197) TokenType.PRINT "print"
11:7 (198-204) TokenType.ID "square"
11:13 (204-205) TokenType.LPAR "("
11:14 (205-206) TokenType.ID "a"
11:15 (206-207) TokenType.RPAR ")"
11:16 (207-208) TokenType.OP_PLUS "+"
11:17 (208-214) TokenType.ID "square"
11:23 (214-215) TokenType.LPAR "("
11:24 (215-216) TokenType.ID "b"
11:25 (216-217) TokenType.RPAR ")"
12:1 (218-259) TokenType.COMMENT "*** actually, the following is also legal"
13:1 (260-265) TokenType.PRINT "print"
13:7 (266-272) TokenType.ID "square"
13:14 (273-274) TokenType.ID "a"
13:16 (275-276) TokenType.OP_PLUS "+"
13:18 (277-283) TokenType.ID "square"
13:25 (284-285) TokenType.ID "b"
16:1 (288-307) TokenType.COMMENT "*** bubblesort? ***"
17:1 (308-313) TokenType.KW_ARRAY "array"
17:7 (314-317) TokenType.ID "arr"
17:10 (317-318) TokenType.LBRAK "["
17:11 (318-319) TokenType.INT_LIT "1"
17:12 (319-321) TokenType.OP_DOTDOT ".."
17:14 (321-323) TokenType.INT_LIT "10"
17:16 (323-324) TokenType.RBRAK "]"
17:17 (324-325) TokenType.SEMI ";"
18:1 (326-329) TokenType.ID "arr"
18:4 (329-330) TokenType.LBRAK "["
18:5 (330-331) TokenType.INT_LIT "1"
18:6 (331-332) TokenType.RBRAK "]"
18:7 (332-333) TokenType.ASSIGN "="
18:8 (333-335) TokenType.INT_LIT "23"
18:10 (335-336) TokenType.SEMI ";"
19:1 (337-340) TokenType.ID "arr"
19:4 (340-341) TokenType.LBRAK "["
19:5 (341-342) TokenType.INT_LIT "2"
19:6 (342-343) TokenType.RBRAK "]"
19:7 (343-344) TokenType.ASSIGN "="
19:8 (344-345) TokenType.INT_LIT "1"
19:9 (345-346) TokenType.SEMI ";"
20:1 (347-350) TokenType.ID "arr"
20:4 (350-351) TokenType.LBRAK "["
20:5 (351-352) TokenType.INT_LIT "3"
20:6 (352-353) TokenType.RBRAK "]"
20:7 (353-354) TokenType.ASSIGN "="
20:8 (354-355) TokenType.INT_LIT "2"
20:9 (355-356) TokenType.SEMI ";"
21:1 (357-360) TokenType.ID "arr"
21:4 (360-361) TokenType.LBRAK "["
21:5 (361-362) TokenType.INT_LIT "4"
21:6 (362-363) TokenType.RBRAK "]"
21:7 (363-364) TokenType.ASSIGN "="
21:8 (364-366) TokenType.INT_LIT "41"
21:10 (366-367) TokenType.SEMI ";"
22:1 (368-371) TokenType.ID "arr"
22:4 (371-372) TokenType.LBRAK "["
22:5 (372-373) TokenType.INT_LIT "5"
22:6 (373-374) TokenType.RBRAK "]"
22:7 (374-375) TokenType.ASSIGN "="
22:8 (375-378) TokenType.INT_LIT "621"
22:11 (378-379) TokenType.SEMI ";"
23:1 (380-383) TokenType.ID "arr"
23:4 (383-384) TokenType.LBRAK "["
23:5 (384-385) TokenType.INT_LIT "6"
23:6 (385-386) TokenType.RBRAK "]"
23:7 (386-387) TokenType.ASSIGN "="
23:8 (387-389) TokenType.INT_LIT "11"
23:10 (389-390) TokenType.SEMI ";"
24:1 (391-394) TokenType.ID "arr"
24:4 (394-395) TokenType.LBRAK "["
24:5 (395-396) TokenType.INT_LIT "7"
24:6 (396-397) TokenType.RBRAK "]"
24:7 (397-398) TokenType.ASSIGN "="
24:8 (398-400) TokenType.INT_LIT "99"
24:10 (400-401) TokenType.SEMI ";"
25:1 (402-405) TokenType.ID "arr"
25:4 (405-406) TokenType.LBRAK "["
25:5 (406-407) TokenType.INT_LIT "8"
25:6 (407-408) TokenType.RBRAK "]"
25:7 (408-409) TokenType.ASSIGN "="
25:8 (409-411) TokenType.INT_LIT "68"
25:10 (411-412) TokenType.SEMI ";"
26:1 (413-416) TokenType.ID "arr"
26:4 (416-417) TokenType.LBRAK "["
26:5 (417-418) TokenType.INT_LIT "9"
26:6 (418-419) TokenType.RBRAK "]"
26:7 (419-420) TokenType.ASSIGN "="
26:8 (420-422) TokenType.INT_LIT "44"
26:10 (422-423) TokenType.SEMI ";"
27:1 (424-427) TokenType.ID "arr"
27:4 (427-428) TokenType.LBRAK "["
27:5 (428-430) TokenType.INT_LIT "10"
27:7 (430-431) TokenType.RBRAK "]"
27:8 (431-432) TokenType.ASSIGN "="
27:9 (432-434) TokenType.INT_LIT "55"
27:11 (434-435) TokenType.SEMI ";"
29:1 (437-440) TokenType.ID "for"
29:5 (441-442) TokenType.ID "i"
29:6 (442-443) TokenType.ASSIGN "="
29:7 (443-444) TokenType.INT_LIT "1"
29:8 (444-446) TokenType.OP_DOTDOT ".."
29:10 (446-447) TokenType.INT_LIT "9"
29:12 (448-450) TokenType.KW_DO "do"
30:3 (453-456) TokenType.ID "for"
30:7 (457-458) TokenType.ID "j"
30:8 (458-459) TokenType.ASSIGN "="
30:9 (459-460) TokenType.INT_LIT "2"
30:10 (460-462) TokenType.OP_DOTDOT ".."
30:12 (462-464) TokenType.INT_LIT "10"
30:15 (465-467) TokenType.KW_DO "do"
31:6 (473-475) TokenType.KW_IF "if"
31:9 (476-479) TokenType.ID "arr"
31:12 (479-480) TokenType.LBRAK "["
31:13 (480-481) TokenType.ID "i"
31:14 (481-482) TokenType.RBRAK "]"
31:15 (482-483) TokenType.OP_GREATER ">"
31:16 (483-486) TokenType.ID "arr"
31:19 (486-487) TokenType.LBRAK "["
31:20 (487-488) TokenType.ID "j"
31:21 (488-489) TokenType.RBRAK "]"
31:23 (490-494) TokenType.KW_THEN "then"
32:9 (503-506) TokenType.ID "arr"
32:12 (506-507) TokenType.LBRAK "["
32:13 (507-508) TokenType.ID "i"
32:14 (508-509) TokenType.RBRAK "]"
32:16 (510-513) TokenType.EXCHANGE "<->"
32:20 (514-517) TokenType.ID "arr"
32:23 (517-518) TokenType.LBRAK "["
32:24 (518-519) TokenType.ID "j"
32:25 (519-520) TokenType.RBRAK "]"
32:26 (520-521) TokenType.SEMI ";"
33:6 (527-530) TokenType.KW_END "end"
33:10 (531-533) TokenType.KW_IF "if"
34:3 (536-539) TokenType.KW_END "end"
34:7 (540-543) TokenType.ID "for"
35:1 (544-547) TokenType.KW_END "end"
35:5 (548-551) TokenType.ID "for"
37:1 (553-556) TokenType.ID "for"
37:5 (557-558) TokenType.ID "i"
37:6 (558-559) TokenType.ASSIGN "="
37:7 (559-560) TokenType.INT_LIT "1"
37:8 (560-562) TokenType.OP_DOTDOT ".."
37:10 (562-564) TokenType.INT_LIT "10"
37:13 (565-567) TokenType.ID "to"
38:4 (571-576) TokenType.PRINT "print"
38:10 (577-580) TokenType.ID "arr"
38:13 (580-581) TokenType.LBRAK "["
38:14 (581-582) TokenType.ID "i"
38:15 (582-583) TokenType.RBRAK "]"
39:1 (584-587) TokenType.KW_END "end"
39:5 (588-591) TokenType.ID "for"
41:1 (596-625) TokenType.COMMENT "*** now weird stuff... tuples"
43:1 (627-632) TokenType.KW_TUPLE "tuple"
43:7 (633-634) TokenType.ID "t"
43:8 (634-635) TokenType.ASSIGN "="
43:9 (635-636) TokenType.INT_LIT "0"
43:10 (636-637) TokenType.OP_COMMA ","
43:11 (637-638) TokenType.INT_LIT "1"
43:12 (638-639) TokenType.OP_COMMA ","
43:13 (639-640) TokenType.INT_LIT "3"
44:1 (641-646) TokenType.KW_TUPLE "tuple"
44:7 (647-649) TokenType.ID "tt"
44:9 (649-650) TokenType.ASSIGN "="
44:10 (650-651) TokenType.INT_LIT "3"
44:11 (651-652) TokenType.OP_COMMA ","
44:12 (652-653) TokenType.INT_LIT "2"
44:13 (653-654) TokenType.OP_COMMA ","
44:14 (654-655) TokenType.INT_LIT "1"
45:1 (656-661) TokenType.KW_TUPLE "tuple"
45:7 (662-665) TokenType.ID "ttt"
45:10 (665-666) TokenType.ASSIGN "="
45:11 (666-667) TokenType.INT_LIT "0"
45:12 (667-668) TokenType.OP_COMMA ","
45:13 (668-669) TokenType.INT_LIT "0"
45:14 (669-670) TokenType.OP_COMMA ","
45:15 (670-671) TokenType.INT_LIT "0"
45:16 (671-672) TokenType.OP_COMMA ","
45:17 (672-673) TokenType.INT_LIT "0"
45:18 (673-674) TokenType.OP_COMMA ","
45:19 (674-675) TokenType.INT_LIT "0"
45:20 (675-676) TokenType.OP_COMMA ","
45:21 (676-677) TokenType.INT_LIT "0"
47:1 (679-682) TokenType.ID "ttt"
47:5 (683-684) TokenType.ASSIGN "="
47:7 (685-686) TokenType.ID "t"
47:8 (686-687) TokenType.OP_COMMA ","
47:9 (687-689) TokenType.ID "tt"
47:12 (690-722) TokenType.COMMENT "*** now ttt contains 0,1,3,3,2,1"
49:1 (724-726) TokenType.ID "tt"
49:3 (726-727) TokenType.ASSIGN "="
49:4 (727-728) TokenType.ID "t"
49:6 (729-754) TokenType.COMMENT "*** now tt contains 0,1,3"
51:1 (756-757) TokenType.ID "a"
51:2 (757-758) TokenType.LBRAK "["
51:3 (758-759) TokenType.INT_LIT "1"
51:4 (759-760) TokenType.RBRAK "]"
51:5 (760-761) TokenType.OP_COMMA ","
51:6 (761-762) TokenType.ID "a"
51:7 (762-763) TokenType.LBRAK "["
51:8 (763-764) TokenType.INT_LIT "2"
51:9 (764-765) TokenType.RBRAK "]"
51:10 (765-766) TokenType.ASSIGN "="
51:11 (766-767) TokenType.ID "a"
51:12 (767-768) TokenType.LBRAK "["
51:13 (768-769) TokenType.INT_LIT "2"
51:14 (769-770) TokenType.RBRAK "]"
51:15 (770-771) TokenType.OP_COMMA ","
51:16 (771-772) TokenType.ID "a"
51:17 (772-773) TokenType.LBRAK "["
51:18 (773-774) TokenType.INT_LIT "1"
51:19 (774-775) TokenType.RBRAK "]"
51:21 (776-810) TokenType.COMMENT "*** exchanged; same as a[1]<->a[2]"
53:1 (812-885) TokenType.COMMENT "*** I am sure we will construct many more interesting examples eventually"
Done.  Scanned 251 tokens.


tests/test4_token_in_garbage.txt
=======================

First token peeked:
1:1 (0-86) TokenType.COMMENT "*** Even though this is mostly garbage, the embedded valid tokens should be picked up."

All tokens: (line:col (begin-end) TYPE "value")
1:1 (0-86) TokenType.COMMENT "*** Even though this is mostly garbage, the embedded valid tokens should be picked up."
2:1 (87-107) TokenType.COMMENT "*** Integer literal:"
3:1-3:2 Warning: unrecognized character "#".
3:2-3:3 Warning: unrecognized character "!".
3:3-3:4 Warning: unrecognized character "#".
3:4-3:5 Warning: unrecognized character "!".
3:5-3:6 Warning: unrecognized character "#".
3:6-3:7 Warning: unrecognized character "!".
3:7 (114-117) TokenType.INT_LIT "123"
3:10-3:11 Warning: unrecognized character "#".
3:11-3:12 Warning: unrecognized character "!".
3:12-3:13 Warning: unrecognized character "#".
3:13-3:14 Warning: unrecognized character "!".
3:14-3:15 Warning: unrecognized character "#".
3:15-3:16 Warning: unrecognized character "!".
4:1 (124-131) TokenType.COMMENT "*** ID:"
5:1-5:2 Warning: unrecognized character "#".
5:2-5:3 Warning: unrecognized character "!".
5:3-5:4 Warning: unrecognized character "#".
5:4-5:5 Warning: unrecognized character "!".
5:5-5:6 Warning: unrecognized character "#".
5:6-5:7 Warning: unrecognized character "!".
5:7 (138-143) TokenType.ID "Hello"
5:12-5:13 Warning: unrecognized character "#".
5:13-5:14 Warning: unrecognized character "!".
5:14-5:15 Warning: unrecognized character "#".
5:15-5:16 Warning: unrecognized character "!".
5:16-5:17 Warning: unrecognized character "#".
5:17-5:18 Warning: unrecognized character "!".
6:1 (150-175) TokenType.COMMENT "*** Keyword and operator:"
7:1-7:2 Warning: unrecognized character "#".
7:2-7:3 Warning: unrecognized character "!".
7:3-7:4 Warning: unrecognized character "#".
7:4-7:5 Warning: unrecognized character "!".
7:5-7:6 Warning: unrecognized character "#".
7:6-7:7 Warning: unrecognized character "!".
7:7 (182-189) TokenType.KW_FOREACH "foreach"
7:14-7:15 Warning: unrecognized character "#".
7:15-7:16 Warning: unrecognized character "!".
7:16-7:17 Warning: unrecognized character "#".
7:17-7:18 Warning: unrecognized character "!".
7:18-7:19 Warning: unrecognized character "#".
7:19-7:20 Warning: unrecognized character "!".
7:20 (195-197) TokenType.OP_LESSEQUAL "<="
7:22-7:23 Warning: unrecognized character "#".
7:23-7:24 Warning: unrecognized character "!".
7:24-7:25 Warning: unrecognized character "#".
7:25-7:26 Warning: unrecognized character "!".
7:26-7:27 Warning: unrecognized character "#".
7:27-7:28 Warning: unrecognized character "!".
Done.  Scanned 8 tokens.


tests/test5_id_or_keyword.txt
=======================

First token peeked:
1:1 (0-86) TokenType.COMMENT "*** IDs should not be confused for keywords even when they start with a valid keyword."

All tokens: (line:col (begin-end) TYPE "value")
1:1 (0-86) TokenType.COMMENT "*** IDs should not be confused for keywords even when they start with a valid keyword."
2:1 (87-99) TokenType.COMMENT "*** Keyword:"
3:1 (100-107) TokenType.KW_FOREACH "foreach"
4:1 (108-115) TokenType.COMMENT "*** ID:"
5:1 (116-124) TokenType.ID "foreach_"
6:1 (125-132) TokenType.COMMENT "*** ID:"
7:1 (133-147) TokenType.ID "foreachforeach"
8:1 (148-155) TokenType.COMMENT "*** ID:"
9:1 (156-169) TokenType.ID "abcforeachabc"
Done.  Scanned 9 tokens.


tests/test6_comments.txt
=======================

First token peeked:
1:1 (0-36) TokenType.COMMENT "********* This is one comment token."

All tokens: (line:col (begin-end) TYPE "value")
1:1 (0-36) TokenType.COMMENT "********* This is one comment token."
2:1 (37-95) TokenType.COMMENT "********* This is another.  Testing keywords in a comment:"
3:1 (96-122) TokenType.COMMENT "********* foreach if else "
4:1 (123-132) TokenType.COMMENT "***abc***"
Done.  Scanned 4 tokens.


tests/test7_tricky_operators.txt
=======================

First token peeked:
1:1 (0-2) TokenType.OP_LESSEQUAL "<="

All tokens: (line:col (begin-end) TYPE "value")
1:1 (0-2) TokenType.OP_LESSEQUAL "<="
1:3 (2-5) TokenType.EXCHANGE "<->"
1:6 (5-7) TokenType.OP_GREATEREQUAL ">="
1:8 (7-9) TokenType.OP_EQUAL "=="
2:1 (10-13) TokenType.EXCHANGE "<->"
2:4 (13-14) TokenType.ASSIGN "="
3:1 (15-17) TokenType.OP_DOTDOT ".."
3:3 (17-18) TokenType.OP_DOT "."
4:1 (19-20) TokenType.OP_DOT "."
4:2 (20-21) TokenType.INT_LIT "1"
4:3 (21-22) TokenType.OP_DOT "."
4:4 (22-23) TokenType.INT_LIT "2"
4:5 (23-24) TokenType.OP_DOT "."
4:6 (24-25) TokenType.INT_LIT "3"
Done.  Scanned 14 tokens.


